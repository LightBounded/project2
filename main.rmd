
Load libraries
```{r}
library(car)
library(MASS)
library(caret)
library(glmnet)
library(olsrr)
```

Functions to check model criteria
```{r}
check_criteria <- function(model) {
    plot(model)

    print(vif(model))

    print(paste("R squared:", summary(model)$r.squared))
    print(paste("R squared adjusted:", summary(model)$adj.r.squared))

    print(paste("AIC:", AIC(model)))
    print(paste("BIC:", BIC(model)))

    mse <- mean((exp(predict(model)) - data$CompTotal)^2)
    print(paste("MSE:", mse))

    rmse <- sqrt(mse)
    print(paste("RMSE:", rmse))
}
```

Load data
```{r}
input_file_path <- "cleaned_data.csv"
data <- read.csv(input_file_path)
nrow(data)
```

Explore data
```{r}
summary(data)
```

Fit a model using the variables we're interested in
```{r}
model <- lm(CompTotal ~ ., data = data)
```

Check 5 assumptions of linear regression
```{r}
plot(model)
```

Our seems to have some influential points, so let's removing them using Cook's distance and refit the model
```{r}
# Find outliers using Cook's distance
cooks_d <- cooks.distance(model)

# Find influential points
n <- nrow(data)
threshold <- 4 / n
influential <- which(cooks_d > threshold)

# Print influential points
print(influential)

# Remove influential points
data <- data[-influential, ]

# Refit the model
model <- lm(CompTotal ~ ., data = data)
plot(model)
```

Remove influential points using leverage and refit the model
```{r}
# Find outliers using leverage
leverage <- hatvalues(model)

# Find influential points
n <- nrow(data)
threshold <- 2 * (ncol(data) + 1) / n
influential <- which(leverage > threshold)

# Print influential points
print(influential)

# Remove influential points
data <- data[-influential, ]

# Refit the model
model <- lm(CompTotal ~ ., data = data)
```

Perform log transformation on compensation to fix problems with homoscedasticity and normality
```{r}
model <- lm(log(CompTotal) ~ ., data = data)
plot(model)
```

Since our model now satisfies the 5 assumptions of linear regression, we can proceed with model selection.

Here's our model's summary
```{r}
check_criteria(model)
```

Although YearsCodePro has a higher GVIF than YearsCode, we'll remove YearsCode instead since we value professional experience more than just general coding experience.

Remove YearsCode and refit the model
```{r}
model <- lm(log(CompTotal) ~ . - YearsCode, data = data)
check_criteria(model)
```

Although nothing has changed, we'll keep the model for the sake of parsimony.

Let's see if we can further improve the model by using stepwise, forward, or backward selection.

Stepwise selection
```{r}
stepwise_model <- step(
    object = lm(log(CompTotal) ~ 1, data = data),
    direction = "both",
    k = log(nrow(data)),
    scope = list(
        lower = ~1,
        upper = model
    )
)
check_criteria(stepwise_model)
```

Forward selection
```{r}
forward_model <- step(
    object = lm(log(CompTotal) ~ 1, data = data),
    direction = "forward",
    k = log(nrow(data)),
    scope = list(
        lower = ~1,
        upper = model
    )
)
check_criteria(forward_model)
```

Backward selection
```{r}
backward_model <- step(
    model,
    direction = "backward",
    k = log(nrow(data)),
)
check_criteria(backward_model)
```

All three methods have resulted in the same model as the one we had before, so we'll keep either one of them.
```{r}
model <- stepwise_model
```

Split the data into training and test sets
```{r}
set.seed(123)
train_index <- createDataPartition(data$CompTotal, p = 0.8, list = FALSE)
train <- data[train_index, ]
test <- data[-train_index, ]
```

Fit a model using the training set
```{r}
nrow(data)
model <- lm(model, data = train)
```

Calculate the R-squared value of the model using the training set
```{r}
r_squared_train <- summary(model)$r.squared
```

Apply the model to the testing set to make predictions
```{r}
predictions <- predict(model, test)
```

Calculate the R-squared value of the model using the predictions and the actual values
```{r}
r_squared_test <- cor(predictions, test$CompTotal)^2
```

Calculate the shrinkage factor
```{r}
shrinkage <- (r_squared_train - r_squared_test)
```

Output the R-squared values and the shrinkage
```{r}
paste("R-squared for Training:", r_squared_train)
paste("R-squared for Testing:", r_squared_test)
paste("Shrinkage:", shrinkage)
```

Interpret the shrinkage value

A shrinkage value <= 0.1 is considered to indicate a reliable model
```{r}
if (shrinkage <= 0.1) {
    print("The model is considered reliable.")
} else {
    print("The model might be overfitting. Consider revisiting the model.")
}
```


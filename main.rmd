
Load libraries
```{r}
library(car)
library(MASS)
library(caret)
library(glmnet)
library(olsrr)
```

Function to check model criteria
```{r}
check_criteria <- function(model) {
    print(vif(model))
    print(paste("R squared:", summary(model)$r.squared))
    print(paste("R squared adjusted:", summary(model)$adj.r.squared))
    print(paste("AIC:", AIC(model)))
    print(paste("BIC:", BIC(model)))
}
```

Load data
```{r}
input_file_path <- "cleaned_data.csv"
data <- read.csv(input_file_path)
nrow(data)
```

Explore data
```{r}
summary(data)
```

Fit a model using the variables we're interested in
```{r}
model <- lm(CompTotal ~ ICorPM + Age + Employment + DevType + OrgSize + EdLevel + RemoteWork + PurchaseInfluence + WorkExp + YearsCode + YearsCodePro + Industry + CodesAsHobbyOnSide + ContributesToOpenSourceOnSide + BootstrapsBusinessOnSide + LearnsCodeProfessionallyOnlineOnSide + DoesAcademicCodingOnSide + DoesFreelanceCodingOnSide, data = data)
```

Check 5 assumptions of linear regression
```{r}
plot(model)
hist(model$residuals)
```

According to our residuals vs. fitted values plot, our model violates the assumption of homoscedasticity. We also see that there is skewness to the right in our residuals histogram, which violates the assumption of normality.

Let's try to fix these issues by performing a log transformation on the response variable.
```{r}
model <- lm(log(CompTotal) ~ ICorPM + Age + Employment + DevType + OrgSize + EdLevel + RemoteWork + PurchaseInfluence + WorkExp + YearsCode + YearsCodePro + Industry + CodesAsHobbyOnSide + ContributesToOpenSourceOnSide + BootstrapsBusinessOnSide + LearnsCodeProfessionallyOnlineOnSide + DoesAcademicCodingOnSide + DoesFreelanceCodingOnSide, data = data)
plot(model)
hist(model$residuals)
```

Our model is now significantly better in terms of homoscedasticity, but could use some improvement in terms of normality.

Let's try to fix this by identifying and removing outliers using Cook's distance.
```{r}
cooks_distance <- cooks.distance(model)
plot(cooks_distance)

outliers <- cooks_distance > 4 / nrow(data)

nrow(data)
data <- data[!outliers, ]
data <- data[complete.cases(data), ]
nrow(data)

model <- lm(log(CompTotal) ~ ICorPM + Age + Employment + DevType + OrgSize + EdLevel + RemoteWork + PurchaseInfluence + WorkExp + YearsCode + YearsCodePro + Industry + CodesAsHobbyOnSide + ContributesToOpenSourceOnSide + BootstrapsBusinessOnSide + LearnsCodeProfessionallyOnlineOnSide + DoesAcademicCodingOnSide + DoesFreelanceCodingOnSide, data = data)
plot(model)
hist(model$residuals)
```

This has significantly improved our model overall. There's no pattern in our residuals vs. fitted values plot, and our residuals are much closer to a normal distribution.

Let's next check for multicollinearity.
```{r}
vif(model)
```

Here we see that YearsCodePro has a GVIF > 5, which is indicative of multicollinearity. However, for this study, we value professional coding experience over general coding experience that includes coding done as a hobby and in school. Therefore, we will keep YearsCodePro in our model and remove YearsCode.
```{r}
model <- lm(log(CompTotal) ~ ICorPM + Age + Employment + DevType + OrgSize + EdLevel + RemoteWork + PurchaseInfluence + WorkExp + YearsCodePro + Industry + CodesAsHobbyOnSide + ContributesToOpenSourceOnSide + BootstrapsBusinessOnSide + LearnsCodeProfessionallyOnlineOnSide + DoesAcademicCodingOnSide + DoesFreelanceCodingOnSide, data = data)
vif(model)
```

Although that the GVIF values of all our variables are < 10, we see that YearsCodePro and WorkExp have GVIF values close to 10. Let's remove WorkExp from our model, since this may include irrelevant, non-coding work experience.
```{r}
model <- lm(log(CompTotal) ~ ICorPM + Age + Employment + DevType + OrgSize + EdLevel + RemoteWork + PurchaseInfluence + YearsCodePro + Industry + CodesAsHobbyOnSide + ContributesToOpenSourceOnSide + BootstrapsBusinessOnSide + LearnsCodeProfessionallyOnlineOnSide + DoesAcademicCodingOnSide + DoesFreelanceCodingOnSide, data = data)
vif(model)
```

Now we see that each GVIF value is < 10, which means that multicollinearity is no longer an issue.

Before we go into model selection, here's the criteria we will using to evaluate our models:

- R squared
- R squared adjusted
- AIC
- BIC
- Domain knowledge

Let's assess our model using the criteria above.

```{r}
check_criteria(model)
```

Our current model is pretty good, but we can do better. Let's try to improve it by performing stepwise, forward, and backward selection using AIC as our selection criteria.

Stepwise selection
```{r}
step_model <- step(lm(log(CompTotal) ~ 1, data = data),
    direction = "both",
    scope = list(upper = model, lower = ~1)
)
check_criteria(step_model)
```

Forward selection
```{r}
forward_model <- step(lm(log(CompTotal) ~ 1, data = data),
    direction = "forward",
    scope = list(upper = model, lower = ~1)
)
check_criteria(forward_model)
```

Backward selection
```{r}
backward_model <- step(model, direction = "backward")
check_criteria(backward_model)
```

All three selection methods have produced the same model. Let's use either of these models as our final model.

R-squared of final model after performing stepwise, forward and backward selection
```{r}
summary(step_model)$r.squared
```

```{r}
model <- step_model
```

Interestingly, our final model has removed CodingAsHobbyOnSide and Employment.

Let's test the reliability of our model using split validation and calculating the shrinkage factor.

```{r}
set.seed(123)

train_index <- createDataPartition(data$CompTotal, p = 0.7, list = FALSE)
train <- data[train_index, ]
test <- data[-train_index, ]

model <- lm(log(CompTotal) ~ ICorPM + Age + DevType + OrgSize + EdLevel + RemoteWork + PurchaseInfluence + YearsCodePro + Industry + ContributesToOpenSourceOnSide + BootstrapsBusinessOnSide + LearnsCodeProfessionallyOnlineOnSide + DoesAcademicCodingOnSide + DoesFreelanceCodingOnSide, data = train)

predictions_train <- predict(model, train)
predictions_test <- predict(model, test)

r_squared_train <- cor(predictions_train, train$CompTotal)^2
r_squared_test <- cor(predictions_test, test$CompTotal)^2

shrinkage_factor <- r_squared_train - r_squared_test
shrinkage_factor
```

Our shrinkage factor is 0.0002889383, which is <= 0.1. Therefore, our model is reliable.

Coefficients of final model after performing stepwise, forward and backward_model
```{r}
coef(model)
```

Let's run Lasso's regression on our model to see if we can improve it further.
```{r}
model <- lm(log(CompTotal) ~ ICorPM + Age + Employment + DevType + OrgSize + EdLevel + RemoteWork + PurchaseInfluence + YearsCodePro + Industry + CodesAsHobbyOnSide + ContributesToOpenSourceOnSide + BootstrapsBusinessOnSide + LearnsCodeProfessionallyOnlineOnSide + DoesAcademicCodingOnSide + DoesFreelanceCodingOnSide, data = data)

y <- data$CompTotal
x <- model.matrix(model, data = data)[, -1]

cv_model <- cv.glmnet(x, y, alpha = 1)

best_lambda <- cv_model$lambda.min

best_model_lasso <- glmnet(x, y, alpha = 1, lambda = best_lambda)

coef(best_model_lasso)

# Get r squared
predictions <- predict(best_model_lasso, x)
r_squared <- cor(predictions, y)^2
r_squared
```

Let's check the reliability of our Lasso model using split validation and calculating the shrinkage factor.

```{r}
set.seed(123)

train_index <- createDataPartition(data$CompTotal, p = 0.7, list = FALSE)
train <- data[train_index, ]
test <- data[-train_index, ]

y_train <- train$CompTotal
x_train <- model.matrix(model, data = train)[, -1]

y_test <- test$CompTotal
x_test <- model.matrix(model, data = test)[, -1]

cv_model <- cv.glmnet(x_train, y_train, alpha = 1)

best_lambda <- cv_model$lambda.min

best_model_lasso <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda)

predictions_train <- predict(best_model_lasso, x_train)
predictions_test <- predict(best_model_lasso, x_test)

r_squared_train <- cor(predictions_train, y_train)^2
r_squared_test <- cor(predictions_test, y_test)^2

shrinkage_factor <- r_squared_train - r_squared_test
shrinkage_factor
```

Our shrinkage factor is 0.0001508322, which is <= 0.1. Therefore, our model is reliable.

Let's run Ridge's regression on our model to see if we can improve it further.
```{r}
y <- data$CompTotal
x <- model.matrix(model, data = data)[, -1]

cv_model <- cv.glmnet(x, y, alpha = 0)

best_lambda <- cv_model$lambda.min

best_model_ridge <- glmnet(x, y, alpha = 0, lambda = best_lambda)

coef(best_model_ridge)

# Get r squared
predictions <- predict(best_model_ridge, x)
r_squared <- cor(predictions, y)^2
r_squared
```

Let's check the reliability of our Ridge model using split validation and calculating the shrinkage factor.

```{r}
set.seed(123)

train_index <- createDataPartition(data$CompTotal, p = 0.7, list = FALSE)
train <- data[train_index, ]
test <- data[-train_index, ]

y_train <- train$CompTotal
x_train <- model.matrix(model, data = train)[, -1]

y_test <- test$CompTotal
x_test <- model.matrix(model, data = test)[, -1]

cv_model <- cv.glmnet(x_train, y_train, alpha = 0)

best_lambda <- cv_model$lambda.min

best_model_ridge <- glmnet(x_train, y_train, alpha = 0, lambda = best_lambda)

predictions_train <- predict(best_model_ridge, x_train)
predictions_test <- predict(best_model_ridge, x_test)

r_squared_train <- cor(predictions_train, y_train)^2
r_squared_test <- cor(predictions_test, y_test)^2

shrinkage_factor <- r_squared_train - r_squared_test
shrinkage_factor
```

Our shrinkage factor is 0.0004936761, which is <= 0.1. Therefore, our model is reliable.

Let's run Elastic Net's regression on our model to see if we can improve it further.
```{r}
y <- data$CompTotal
x <- model.matrix(model, data = data)[, -1]

cv_model <- cv.glmnet(x, y, alpha = 0.5)

best_lambda <- cv_model$lambda.min

best_model_elastic_net <- glmnet(x, y, alpha = 0.5, lambda = best_lambda)

coef(best_model_elastic_net)

# Get r squared
predictions <- predict(best_model_elastic_net, x)

r_squared <- cor(predictions, y)^2

r_squared
```

Let's check the reliability of our Elastic Net model using split validation and calculating the shrinkage factor.

```{r}
set.seed(123)

train_index <- createDataPartition(data$CompTotal, p = 0.7, list = FALSE)
train <- data[train_index, ]
test <- data[-train_index, ]

y_train <- train$CompTotal
x_train <- model.matrix(model, data = train)[, -1]

y_test <- test$CompTotal
x_test <- model.matrix(model, data = test)[, -1]

cv_model <- cv.glmnet(x_train, y_train, alpha = 0.5)

best_lambda <- cv_model$lambda.min

best_model_elastic_net <- glmnet(x_train, y_train, alpha = 0.5, lambda = best_lambda)

predictions_train <- predict(best_model_elastic_net, x_train)
predictions_test <- predict(best_model_elastic_net, x_test)

r_squared_train <- cor(predictions_train, y_train)^2
r_squared_test <- cor(predictions_test, y_test)^2

shrinkage_factor <- r_squared_train - r_squared_test
shrinkage_factor
```

Our shrinkage factor is 0.0001802046, which is <= 0.1. Therefore, our model is reliable.

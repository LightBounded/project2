
Load libraries
```{r}
library(car)
library(MASS)
library(caret)
library(glmnet)
library(olsrr)
```

Function to check model criteria
```{r}
check_criteria <- function(model) {
    print(paste("R squared:", summary(model)$r.squared))
    print(paste("R squared adjusted:", summary(model)$adj.r.squared))
    print(paste("AIC:", AIC(model)))
    print(paste("BIC:", BIC(model)))
}
```

Load data
```{r}
input_file_path <- "cleaned_data.csv"
data <- read.csv(input_file_path)
nrow(data)
```

Explore data
```{r}
summary(data)
```

Fit a model using the variables we're interested in
```{r}
model <- lm(CompTotal ~ ICorPM + Age + Employment + DevType + OrgSize + EdLevel + RemoteWork + PurchaseInfluence + WorkExp + YearsCode + YearsCodePro + Industry + CodesAsHobbyOnSide + ContributesToOpenSourceOnSide + BootstrapsBusinessOnSide + LearnsCodeProfessionallyOnlineOnSide + DoesAcademicCodingOnSide, data = data)
```

Check 5 assumptions of linear regression
```{r}
plot(model)
hist(model$residuals)
```

According to our residuals vs. fitted values plot, our model violates the assumption of homoscedasticity. We also see that there is skewness to the right in our residuals histogram, which violates the assumption of normality.

Let's try to fix these issues by performing a log transformation on the response variable.
```{r}
model <- lm(log(CompTotal) ~ ICorPM + Age + Employment + DevType + OrgSize + EdLevel + RemoteWork + PurchaseInfluence + WorkExp + YearsCode + YearsCodePro + Industry + CodesAsHobbyOnSide + ContributesToOpenSourceOnSide + BootstrapsBusinessOnSide + LearnsCodeProfessionallyOnlineOnSide + DoesAcademicCodingOnSide, data = data)
plot(model)
hist(model$residuals)
```

Our model is now significantly better in terms of homoscedasticity, but could use some improvement in terms of normality.

Let's try to fix this by identifying and removing outliers using Cook's distance.
```{r}
cooks_distance <- cooks.distance(model)
plot(cooks_distance)

outliers <- cooks_distance > 4 / nrow(data)

nrow(data)
data <- data[!outliers, ]
data <- na.omit(data)
nrow(data)

model <- lm(log(CompTotal) ~ ICorPM + Age + Employment + DevType + OrgSize + EdLevel + RemoteWork + PurchaseInfluence + WorkExp + YearsCode + YearsCodePro + Industry + CodesAsHobbyOnSide + ContributesToOpenSourceOnSide + BootstrapsBusinessOnSide + LearnsCodeProfessionallyOnlineOnSide + DoesAcademicCodingOnSide, data = data)
plot(model)
hist(model$residuals)
```

This has significantly improved our model overall. There's no pattern in our residuals vs. fitted values plot, and our residuals are much closer to a normal distribution.

Let's next check for multicollinearity.
```{r}
vif(model)
```

Here we see that YearsCodePro has a GVIF > 5, which is indicative of multicollinearity. However, for this study, we value professional coding experience over general coding experience that includes coding done as a hobby and in school. Therefore, we will keep YearsCodePro in our model and remove YearsCode.
```{r}
model <- lm(log(CompTotal) ~ ICorPM + Age + Employment + DevType + OrgSize + EdLevel + RemoteWork + PurchaseInfluence + WorkExp + YearsCodePro + Industry + CodesAsHobbyOnSide + ContributesToOpenSourceOnSide + BootstrapsBusinessOnSide + LearnsCodeProfessionallyOnlineOnSide + DoesAcademicCodingOnSide, data = data)
vif(model)
```

Although that the GVIF values of all our variables are < 10, we see that YearsCodePro and WorkExp have GVIF values close to 10. Let's remove WorkExp from our model, since this may include irrelevant, non-coding work experience.
```{r}
model <- lm(log(CompTotal) ~ ICorPM + Age + Employment + DevType + OrgSize + EdLevel + RemoteWork + PurchaseInfluence + YearsCodePro + Industry + CodesAsHobbyOnSide + ContributesToOpenSourceOnSide + BootstrapsBusinessOnSide + LearnsCodeProfessionallyOnlineOnSide + DoesAcademicCodingOnSide, data = data)
vif(model)
```

Now we see that each GVIF value is < 10, which means that multicollinearity is no longer an issue.

Before we go into model selection, here's the criteria we will using to evaluate our models:

- R squared
- R squared adjusted
- AIC
- BIC
- Domain knowledge

Let's assess our model using the criteria above.

```{r}
check_criteria(model)
```

Our current model is pretty good, but we can do better. Let's try to improve it by performing stepwise, forward, and backward selection using AIC as our selection criteria.

Stepwise selection
```{r}
step_model <- step(lm(log(CompTotal) ~ 1, data = data),
    direction = "both",
    scope = list(upper = model, lower = ~1)
)
check_criteria(step_model)
```

Forward selection
```{r}
forward_model <- step(lm(log(CompTotal) ~ 1, data = data),
    direction = "forward",
    scope = list(upper = model, lower = ~1)
)
check_criteria(forward_model)
```

Backward selection
```{r}
backward_model <- step(model, direction = "backward")
check_criteria(backward_model)
```

All three selection methods have produced models that are the same as our original model. Therefore, we will keep our original model.

Let's test the reliability of our model using split validation and calculating the shrinkage factor.

```{r}
set.seed(123)

train_index <- createDataPartition(data$CompTotal, p = 0.7, list = FALSE)
train <- data[train_index, ]
test <- data[-train_index, ]

model <- lm(log(CompTotal) ~ ICorPM + Age + Employment + DevType + OrgSize + EdLevel + RemoteWork + PurchaseInfluence + YearsCodePro + Industry, data = train)

predictions_train <- predict(model, train)
predictions_test <- predict(model, test)

r_squared_train <- cor(predictions_train, train$CompTotal)^2
r_squared_test <- cor(predictions_test, test$CompTotal)^2

shrinkage_factor <- r_squared_train - r_squared_test
shrinkage_factor
```

Our shrinkage factor is 0.03413445, which is <= 0.1. Therefore, our model is reliable.

